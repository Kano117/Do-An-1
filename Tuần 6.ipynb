{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuần 6(10/10 - 16/10)\n",
    "Để nhận diện được chuyển động thành một câu hoàn chính, ta dùng mediapipe tìm các keypoint rồi nồi các keypoint lại tạo thành bộ khung, dựa vào góc độ khung xương tay để nhận diện\n",
    "\n",
    "Sẽ cần dùng 2 model, một model nhận diện từng bức ảnh và model còn lại dùng đầu ra nhận diện kết quả cuối cùng \n",
    "\n",
    "### Dùng Deep Learning nhận diện kí hiệu tách nền ( tham khảo từ [Sign-Language-Interpreter-using-Deep-Learning](https://github.com/harshbg/Sign-Language-Interpreter-using-Deep-Learning?tab=readme-ov-file#general-info) )\n",
    "\n",
    ">**Ý tưởng :** Dữ liệu đầu vào sẽ được phân tích qua các lớp ẩn, phân tích từng lớp và cho ra kết quả cuối cùng, dũng kỹ thuật tách nền tách ra bóng của kí tự tay làm dữ liệu đầu vào, ví dụ:\n",
    "\n",
    "Ảnh đầu vào là kí hiệu i love u\n",
    "\n",
    " ![demo2.gif](https://raw.githubusercontent.com/harshbg/Sign-Language-Interpreter-using-Deep-Learning/refs/heads/master/img/demo2.gif)\n",
    "\n",
    "Sẽ có các lớp ẩn phân tích các lớp ban đầu như hình dạng tay, dữ liệu đầu ra sẽ đi đến bước tiếp theo nhận diện vị trí các ngón tay, …, và có nhiều lớp ẩn nhận diện khác phức tạp và chi tiết hơn, càng nhiều lớp thì kết quả càng chính xác\n",
    "\n",
    "![Screenshot 2024-10-16 191021.png](https://raw.githubusercontent.com/Kano117/Do-An-1/refs/heads/main/img/Screenshot%202024-10-16%20191021.png)\n",
    "\n",
    "Ngôn ngữ sử dụng là Python, thư viện Pytorch và TensorFlow\n",
    "\n",
    "**Ưu điểm:** càng nhiều lớp ẩn thì dữ liệu càng chính xác, nếu đầu tư nhiều nhận chất lượng nhận diện chính xác sẽ cao\n",
    "\n",
    "**Nhược:** cần máy chủ mạnh và lượng dữ liệu quá lớn, các kí hiệu tượng tự nhau sẽ không chính xác, ví dụ kí tự M và N trong bảng chữ cái\n",
    "\n",
    "Có vẻ như chưa nhận diện được các câu cần nhiều chuyển động, chỉ mới ở mức nhận diện các kí hiệu tĩnh\n",
    "\n",
    "### Dùng deep learning, mediapipe và RNN (Recurrent neural network)\n",
    "\n",
    "Tài liệu tham khảo: [Recurrent neural network](https://nttuan8.com/bai-13-recurrent-neural-network/)\n",
    "\n",
    "[Mạng Recurrent Neural Network (RNN) với Tensorflow](https://www.youtube.com/watch?v=BpQQdajMCf0)\n",
    "\n",
    "[Nhận diện hành vi con người bằng Mediapipe Pose và LSTM Model](https://www.youtube.com/watch?v=_52-kz08LvU)\n",
    "\n",
    "[Real-Time Hand Tracking and Gesture Recognition with MediaPipe: Rerun Showcase](https://towardsdatascience.com/real-time-hand-tracking-and-gesture-recognition-with-mediapipe-rerun-showcase-9ec57cb0c831)\n",
    "\n",
    "[Sign Language Recognition - using MediaPipe & DTW](https://data-ai.theodo.com/blog-technique/sign-language-recognition-using-mediapipe)\n",
    "\n",
    "[ActionAI: Custom Tracking & MultiPerson Activity Recognition](https://www.hackster.io/actionai/actionai-custom-tracking-multiperson-activity-recognition-fa5cb5)\n",
    "\n",
    "Để nhân diện được các kí tự nhiều chuyển động phức tạp, ta sẽ dùng Deep learning để nhận diện các khung hình tách ra từ video hoặc hình ảnh từ webcam, nhưng ta sẽ dùng thêm mediapipe giúp nhận diện các keypoint để tăng độ chính xác vị trí các khớp tay.\n",
    "\n",
    ">**Ý tưởng:** RNN  giúp nhận diện kí tự ta đã biểu diễn thông qua trình tự các frame, từ đó giúp nhận diện được >các kí tự động, đòi hỏi nhiều thao tác (RNN one to one)\n",
    "\n",
    "Chia hành động thành nhiều frame khác nhau, các ảnh sẽ được cho qua model lấy ra output, sau đó dùng nó làm input kèm với ảnh frame của trạng thái tiếp theo\n",
    "![demo2](https://raw.githubusercontent.com/Kano117/Do-An-1/refs/heads/main/img/Tuan6/model-2.png)\n",
    "\n",
    "Đối với từng frame, ta lại sử dụng model thứ 2 dùng CNN (Convolutional Neural Network) để nhận diện dành động của bức hình lúc đó, sau đó tổng hợp lại cho RNN nhận diện kí tự cho tổng thể\n",
    "\n",
    "**Nhược:** xuất hiện vanishing gradient, là khi thông tin từ layer càng về sau càng không nhận được dữ liệu từ layer ở xa, nghĩa là model chỉ học đc từ các trạng thái ở gần nó ( Short Term Memory)\n",
    "\n",
    "→ dùng LSTM\n",
    "\n",
    "LSTM có các cổng kiểm soát luồng thông tin giúp nó quyết định được những gì cần nhớ,những gì cần quên và những gì cần xuất ra \n",
    "\n",
    "![demoo](https://raw.githubusercontent.com/Kano117/Do-An-1/refs/heads/main/img/Tuan6/1538736f-fc4c-4069-bb4f-afe69b4d85cf.png)\n",
    "Trong quá trình tìm hiểu LSTM có tìm được thông tin về Intelligent Video Analytics (IVA) song thấy phương pháp này chỉ nhận diện vật thể dạng tĩnh không áp dụng nhận diện chuyển động kí tự nhiều chuyển động phức tạp được\n",
    "\n",
    "![minhhoaa](https://raw.githubusercontent.com/Kano117/Do-An-1/refs/heads/main/img/Tuan6/ai-video-search-05.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
